{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mountain Car\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n",
    "\n",
    "## A Mountain Car Environment\n",
    "\n",
    "[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n",
    "\n",
    "The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n",
    "\n",
    "| Value | Meaning |\n",
    "|---|---|\n",
    "| 0 | Accelerate to the left |\n",
    "| 1 | Do not accelerate |\n",
    "| 2 | Accelerate to the right |\n",
    "\n",
    "The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Observation space consists of just two values:\n",
    "\n",
    "| Num | Observation  | Min | Max |\n",
    "|-----|--------------|-----|-----|\n",
    "|  0  | Car Position | -1.2| 0.6 |\n",
    "|  1  | Car Velocity | -0.07 | 0.07 |\n",
    "\n",
    "Reward system for the mountain car is rather tricky:\n",
    "\n",
    " * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n",
    " * Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "\n",
    "Episode terminates if the car position is more than 0.5, or episode length is greater than 200.\n",
    "## Instructions\n",
    "\n",
    "Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n",
    "\n",
    "> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!pip install gym \n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a cartpole environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukmanaliyu/miniconda3/envs/arewads/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/lukmanaliyu/miniconda3/envs/arewads/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   env.render()\n",
    "   env.step(env.action_space.sample())\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, step function returns us back current observations, reward function, and the done flag that indicates whether it makes sense to continue the simulation or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02290763 -0.20757952 -0.01469015  0.31233308] -> 1.0\n",
      "[ 0.01875604 -0.0122514  -0.00844349  0.01505378] -> 1.0\n",
      "[ 0.01851101 -0.20725125 -0.00814241  0.30506077] -> 1.0\n",
      "[ 0.01436599 -0.40225622 -0.0020412   0.5951647 ] -> 1.0\n",
      "[ 0.00632086 -0.5973495   0.0098621   0.88720393] -> 1.0\n",
      "[-0.00562613 -0.40236285  0.02760618  0.59763753] -> 1.0\n",
      "[-0.01367339 -0.20763783  0.03955892  0.3137765 ] -> 1.0\n",
      "[-0.01782614 -0.01310109  0.04583446  0.03382697] -> 1.0\n",
      "[-0.01808817  0.18133463  0.04651099 -0.2440497 ] -> 1.0\n",
      "[-0.01446147  0.37576246  0.04163    -0.52170676] -> 1.0\n",
      "[-0.00694622  0.18008001  0.03119587 -0.2162017 ] -> 1.0\n",
      "[-0.00334462  0.37474242  0.02687183 -0.49888316] -> 1.0\n",
      "[ 0.00415023  0.5694754   0.01689417 -0.7829778 ] -> 1.0\n",
      "[ 0.01553973  0.37412542  0.00123461 -0.48502797] -> 1.0\n",
      "[ 0.02302224  0.5692299  -0.00846595 -0.7773215 ] -> 1.0\n",
      "[ 0.03440684  0.3742254  -0.02401238 -0.4873142 ] -> 1.0\n",
      "[ 0.04189135  0.17945035 -0.03375866 -0.2022948 ] -> 1.0\n",
      "[ 0.04548036  0.37503842 -0.03780456 -0.5054329 ] -> 1.0\n",
      "[ 0.05298112  0.57067215 -0.04791322 -0.8097857 ] -> 1.0\n",
      "[ 0.06439456  0.7664167  -0.06410893 -1.1171467 ] -> 1.0\n",
      "[ 0.0797229   0.9623187  -0.08645187 -1.4292312 ] -> 1.0\n",
      "[ 0.09896927  1.1583953  -0.11503649 -1.7476329 ] -> 1.0\n",
      "[ 0.12213718  1.3546214  -0.14998914 -2.0737727 ] -> 1.0\n",
      "[ 0.14922962  1.1613072  -0.1914646  -1.8309888 ] -> 1.0\n",
      "[ 0.17245576  1.357965   -0.22808437 -2.1765323 ] -> 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukmanaliyu/miniconda3/envs/arewads/lib/python3.10/site-packages/gym/envs/classic_control/cartpole.py:211: UserWarning: \u001b[33mWARN: You are calling render method without specifying any render mode. You can specify the render_mode at initialization, e.g. gym(\"CartPole-v1\", render_mode=\"rgb_array\")\u001b[0m\n",
      "  gym.logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "   env.render()\n",
    "   obs, rew, done, info,_ = env.step(env.action_space.sample())\n",
    "   print(f\"{obs} -> {rew}\")\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get min and max value of those numbers:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x):\n",
    "    return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also explore other discretization method using bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample bins for interval (-5,5) with 10 bins\n",
      " [-5. -4. -3. -2. -1.  0.  1.  2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "def create_bins(i,num):\n",
    "    return np.arange(num+1)*(i[1]-i[0])/num+i[0]\n",
    "\n",
    "print(\"Sample bins for interval (-5,5) with 10 bins\\n\",create_bins((-5,5),10))\n",
    "\n",
    "ints = [(-5,5),(-2,2),(-0.5,0.5),(-2,2)] # intervals of values for each parameter\n",
    "nbins = [20,20,10,10] # number of bins for each parameter\n",
    "bins = [create_bins(ints[i],nbins[i]) for i in range(4)]\n",
    "\n",
    "def discretize_bins(x):\n",
    "    return tuple(np.digitize(x[i],bins[i]) for i in range(4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run a short simulation and observe those discrete environment values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, -3)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, -1, 2)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, -3)\n",
      "(0, 0, -1, 0)\n",
      "(0, 0, -1, 2)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 2)\n",
      "(0, 0, 0, 0)\n",
      "(0, 0, 0, 2)\n",
      "(0, -1, 0, 5)\n",
      "(0, -2, 1, 8)\n",
      "(0, -1, 2, 5)\n",
      "(0, -2, 3, 8)\n",
      "(0, -1, 5, 5)\n",
      "(0, -2, 6, 8)\n",
      "(0, -1, 8, 5)\n",
      "(0, 0, 9, 3)\n",
      "(0, 0, 10, 0)\n",
      "(0, 0, 10, 3)\n",
      "(0, -1, 11, 7)\n",
      "(0, -2, 12, 10)\n",
      "(0, -1, 14, 7)\n",
      "(0, 0, 16, 5)\n",
      "(0, -1, 17, 8)\n",
      "(0, -2, 18, 12)\n",
      "(0, -1, 21, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3803/2541155765.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "   #env.render()\n",
    "   obs, rew, done, info,_ = env.step(env.action_space.sample())\n",
    "   #print(discretize_bins(obs))\n",
    "   print(discretize(obs))\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q-Table Structure**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "actions = (0,1)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's Start Q-Learning!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3803/2541155765.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return tuple((x/np.array([0.25, 0.25, 0.01, 0.1])).astype(np.int))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# == do the simulation ==\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> 15\u001b[0m     s \u001b[39m=\u001b[39m discretize(obs)\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m random\u001b[39m.\u001b[39mrandom()\u001b[39m<\u001b[39mepsilon:\n\u001b[1;32m     17\u001b[0m         \u001b[39m# exploitation - chose the action according to Q-Table probabilities\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         v \u001b[39m=\u001b[39m probs(np\u001b[39m.\u001b[39marray(qvalues(s)))\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36mdiscretize\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdiscretize\u001b[39m(x):\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtuple\u001b[39m((x\u001b[39m/\u001b[39;49mnp\u001b[39m.\u001b[39;49marray([\u001b[39m0.25\u001b[39;49m, \u001b[39m0.25\u001b[39;49m, \u001b[39m0.01\u001b[39;49m, \u001b[39m0.1\u001b[39;49m]))\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mint))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (4,) "
     ]
    }
   ],
   "source": [
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v\n",
    "\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "for epoch in range(100000):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    cum_reward=0\n",
    "    # == do the simulation ==\n",
    "    while not done:\n",
    "        s = discretize(obs)\n",
    "        if random.random()<epsilon:\n",
    "            # exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions,weights=v)[0]\n",
    "        else:\n",
    "            # exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "\n",
    "        obs, rew, done, info = env.step(a)\n",
    "        cum_reward+=rew\n",
    "        ns = discretize(obs)\n",
    "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (rew + gamma * max(qvalues(ns)))\n",
    "    cum_rewards.append(cum_reward)\n",
    "    rewards.append(cum_reward)\n",
    "    # == Periodically print results and calculate average reward ==\n",
    "    if epoch%5000==0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards=[]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it is not possible to tell anything, because due to the nature of stochastic training process the length of training sessions varies greatly. To make more sense of this graph, we can calculate running average over series of experiments, let's say 100. This can be done conveniently using np.convolve:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Hyperparameters and Seeing the Result in Action\n",
    "\n",
    "Now it would be interesting to actually see how the trained model behaves. Let's run the simulation, and we will be following the same action selection strategy as during training: sampling according to the probability distribution in Q-Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "   s = discretize(obs)\n",
    "   env.render()\n",
    "   v = probs(np.array(qvalues(s)))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   obs,_,done,_ = env.step(a)\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving result to an animated GIF\n",
    "If you want to impress your friends, you may want to send them the animated GIF picture of the balancing pole. To do this, we can invoke env.render to produce an image frame, and then save those to animated GIF using PIL library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "obs = env.reset()\n",
    "done = False\n",
    "i=0\n",
    "ims = []\n",
    "while not done:\n",
    "   s = discretize(obs)\n",
    "   img=env.render(mode='rgb_array')\n",
    "   ims.append(Image.fromarray(img))\n",
    "   v = probs(np.array([Qbest.get((s,a),0) for a in actions]))\n",
    "   a = random.choices(actions,weights=v)[0]\n",
    "   obs,_,done,_ = env.step(a)\n",
    "   i+=1\n",
    "env.close()\n",
    "ims[0].save('images/cartpole-balance.gif',save_all=True,append_images=ims[1::2],loop=0,duration=5)\n",
    "print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arewads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
