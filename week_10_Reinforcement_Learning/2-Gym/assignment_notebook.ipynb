{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mountain Car\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n",
    "\n",
    "## A Mountain Car Environment\n",
    "\n",
    "[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n",
    "\n",
    "The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n",
    "\n",
    "| Value | Meaning |\n",
    "|---|---|\n",
    "| 0 | Accelerate to the left |\n",
    "| 1 | Do not accelerate |\n",
    "| 2 | Accelerate to the right |\n",
    "\n",
    "The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Observation space consists of just two values:\n",
    "\n",
    "| Num | Observation  | Min | Max |\n",
    "|-----|--------------|-----|-----|\n",
    "|  0  | Car Position | -1.2| 0.6 |\n",
    "|  1  | Car Velocity | -0.07 | 0.07 |\n",
    "\n",
    "Reward system for the mountain car is rather tricky:\n",
    "\n",
    " * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n",
    " * Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "\n",
    "Episode terminates if the car position is more than 0.5, or episode length is greater than 200.\n",
    "## Instructions\n",
    "\n",
    "Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n",
    "\n",
    "> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a mountain car environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\",render_mode = \"human\")\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the environment works, let's run a short simulation for 100 steps. At each step, we provide one of the actions to be taken - in this simulation we just randomly select an action from action_space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\",render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "   action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "   #if terminated or truncated:\n",
    "     # observation, info = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During simulation, we need to get observations in order to decide how to act. In fact, step function returns us back current observations, reward function, and the done flag that indicates whether it makes sense to continue the simulation or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"{observation} -> {reward}\")\n",
    "   \n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get min and max value of those numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(x):\n",
    "    return tuple(((x - np.array([-1.2, -0.07])) / np.array([0.1, 0.01])).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode = \"human\")\n",
    "observation, info = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not terminated and not truncated:\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    print(discretize(observation))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = {}\n",
    "actions = (0,1)\n",
    "\n",
    "def qvalues(state):\n",
    "    return [Q.get((state,a),0) for a in actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "alpha = 0.3\n",
    "gamma = 0.9\n",
    "epsilon = 0.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs(v,eps=1e-4):\n",
    "    v = v-v.min()+eps\n",
    "    v = v/v.sum()\n",
    "    return v\n",
    "\n",
    "Qmax = 0\n",
    "cum_rewards = []\n",
    "rewards = []\n",
    "for epoch in range(100000):\n",
    "    observation, info = env.reset()\n",
    "    cum_reward=0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    # == do the simulation ==\n",
    "    while not terminated and not truncated:\n",
    "        s = discretize(observation)\n",
    "        if random.random()<epsilon:\n",
    "        # exploitation - chose the action according to Q-Table probabilities\n",
    "            v = probs(np.array(qvalues(s)))\n",
    "            a = random.choices(actions,weights=v)[0]\n",
    "        else:\n",
    "        # exploration - randomly chose the action\n",
    "            a = np.random.randint(env.action_space.n)\n",
    "\n",
    "        observation, reward, terminated, truncated, info = env.step(a)\n",
    "        cum_reward+=reward\n",
    "        ns = discretize(observation)\n",
    "        Q[(s,a)] = (1 - alpha) * Q.get((s,a),0) + alpha * (reward + gamma * max(qvalues(ns)))\n",
    "    \n",
    "                \n",
    "    cum_rewards.append(cum_reward)\n",
    "    rewards.append(cum_reward)\n",
    "    # == Periodically print results and calculate average reward ==\n",
    "    if epoch%5000==0:\n",
    "        print(f\"{epoch}: {np.average(cum_rewards)}, alpha={alpha}, epsilon={epsilon}\")\n",
    "        if np.average(cum_rewards) > Qmax:\n",
    "            Qmax = np.average(cum_rewards)\n",
    "            Qbest = Q\n",
    "        cum_rewards=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(x,window):\n",
    "    return np.convolve(x,np.ones(window)/window,mode='valid')\n",
    "\n",
    "plt.plot(running_average(rewards,100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving result to an animated GIF\n",
    "If you want to impress your friends, you may want to send them the animated GIF picture of the mountain car. To do this, we can invoke env.render to produce an image frame, and then save those to animated GIF using PIL library:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arewads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
