{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Mountain Car\n",
    "\n",
    "[OpenAI Gym](http://gym.openai.com) has been designed in such a way that all environments provide the same API - i.e. the same methods `reset`, `step` and `render`, and the same abstractions of **action space** and **observation space**. Thus is should be possible to adapt the same reinforcement learning algorithms to different environments with minimal code changes.\n",
    "\n",
    "## A Mountain Car Environment\n",
    "\n",
    "[Mountain Car environment](https://gym.openai.com/envs/MountainCar-v0/) contains a car stuck in a valley:\n",
    "\n",
    "The goal is to get out of the valley and capture the flag, by doing at each step one of the following actions:\n",
    "\n",
    "| Value | Meaning |\n",
    "|---|---|\n",
    "| 0 | Accelerate to the left |\n",
    "| 1 | Do not accelerate |\n",
    "| 2 | Accelerate to the right |\n",
    "\n",
    "The main trick of this problem is, however, that the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n",
    "\n",
    "Observation space consists of just two values:\n",
    "\n",
    "| Num | Observation  | Min | Max |\n",
    "|-----|--------------|-----|-----|\n",
    "|  0  | Car Position | -1.2| 0.6 |\n",
    "|  1  | Car Velocity | -0.07 | 0.07 |\n",
    "\n",
    "Reward system for the mountain car is rather tricky:\n",
    "\n",
    " * Reward of 0 is awarded if the agent reached the flag (position = 0.5) on top of the mountain.\n",
    " * Reward of -1 is awarded if the position of the agent is less than 0.5.\n",
    "\n",
    "Episode terminates if the car position is more than 0.5, or episode length is greater than 200.\n",
    "## Instructions\n",
    "\n",
    "Adapt our reinforcement learning algorithm to solve the mountain car problem. Start with existing [notebook.ipynb](notebook.ipynb) code, substitute new environment, change state discretization functions, and try to make existing algorithm to train with minimal code modifications. Optimize the result by adjusting hyperparameters.\n",
    "\n",
    "> **Note**: Hyperparameters adjustment is likely to be needed to make algorithm converge. \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
